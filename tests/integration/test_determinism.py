"""Integration tests for byte-identical reproducibility.

These tests verify that the determinism enforcement module delivers
on the "byte-identical" promise across:
- Multiple runs with same seed
- Different platforms (Linux, macOS)
- PDF generation
- Audit manifests
"""

import json

import numpy as np
import pandas as pd
import pytest
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

from glassalpha.config import load_config_from_file
from glassalpha.pipeline.audit import AuditPipeline
from glassalpha.utils.determinism import (
    compute_file_hash,
    deterministic,
    validate_deterministic_environment,
    verify_deterministic_output,
)


@pytest.fixture
def tiny_binary_data():
    """Create tiny dataset for fast determinism testing."""
    np.random.seed(42)
    X = np.random.randn(100, 5)
    y = (X[:, 0] + X[:, 1] > 0).astype(int)
    feature_names = [f"feature_{i}" for i in range(5)]
    return pd.DataFrame(X, columns=feature_names), pd.Series(y, name="target")


@pytest.fixture
def simple_config(tmp_path, tiny_binary_data):
    """Create simple audit config for determinism testing."""
    X, y = tiny_binary_data

    # Save data
    data_path = tmp_path / "data.csv"
    df = X.copy()
    df["target"] = y
    df.to_csv(data_path, index=False)

    # Create config (valid AuditConfig schema)
    config = {
        "data": {
            "dataset": "custom",  # Required for external files
            "path": str(data_path),
            "target_column": "target",
        },
        "model": {
            "type": "logistic_regression",
        },
        "reproducibility": {
            "random_seed": 42,
        },
    }

    config_path = tmp_path / "config.yaml"
    import yaml

    with config_path.open("w") as f:
        yaml.dump(config, f)

    return config_path, config


class TestDeterministicContext:
    """Test deterministic context manager."""

    def test_random_state_is_deterministic(self):
        """Test Python random module determinism."""
        import random

        with deterministic(seed=42):
            values1 = [random.random() for _ in range(10)]

        with deterministic(seed=42):
            values2 = [random.random() for _ in range(10)]

        assert values1 == values2

    def test_numpy_random_is_deterministic(self):
        """Test NumPy random determinism."""
        with deterministic(seed=42):
            arr1 = np.random.rand(10)

        with deterministic(seed=42):
            arr2 = np.random.rand(10)

        assert np.array_equal(arr1, arr2)

    def test_sklearn_is_deterministic(self):
        """Test sklearn model training determinism."""
        X = np.random.randn(100, 5)
        y = (X[:, 0] > 0).astype(int)

        with deterministic(seed=42):
            model1 = LogisticRegression(random_state=42, max_iter=100)
            model1.fit(X, y)
            preds1 = model1.predict_proba(X)

        with deterministic(seed=42):
            model2 = LogisticRegression(random_state=42, max_iter=100)
            model2.fit(X, y)
            preds2 = model2.predict_proba(X)

        assert np.allclose(preds1, preds2)

    def test_environment_variables_set(self):
        """Test environment variables are set correctly."""
        import os

        with deterministic(seed=123, strict=True):
            assert os.environ.get("PYTHONHASHSEED") == "123"
            assert os.environ.get("OMP_NUM_THREADS") == "1"
            assert os.environ.get("OPENBLAS_NUM_THREADS") == "1"
            assert os.environ.get("MKL_NUM_THREADS") == "1"

    def test_environment_restored_after_context(self):
        """Test environment is restored after context exit."""
        import os

        original_hashseed = os.environ.get("PYTHONHASHSEED")

        with deterministic(seed=99):
            assert os.environ.get("PYTHONHASHSEED") == "99"

        # Should be restored
        assert os.environ.get("PYTHONHASHSEED") == original_hashseed


class TestFileHashing:
    """Test file hashing utilities."""

    def test_identical_files_have_same_hash(self, tmp_path):
        """Test identical files produce same hash."""
        content = b"test content for hashing"

        file1 = tmp_path / "file1.txt"
        file2 = tmp_path / "file2.txt"

        file1.write_bytes(content)
        file2.write_bytes(content)

        hash1 = compute_file_hash(file1)
        hash2 = compute_file_hash(file2)

        assert hash1 == hash2

    def test_different_files_have_different_hash(self, tmp_path):
        """Test different files produce different hashes."""
        file1 = tmp_path / "file1.txt"
        file2 = tmp_path / "file2.txt"

        file1.write_text("content 1")
        file2.write_text("content 2")

        hash1 = compute_file_hash(file1)
        hash2 = compute_file_hash(file2)

        assert hash1 != hash2

    def test_verify_deterministic_output(self, tmp_path):
        """Test verification of deterministic outputs."""
        content = b"deterministic content"

        file1 = tmp_path / "out1.txt"
        file2 = tmp_path / "out2.txt"

        file1.write_bytes(content)
        file2.write_bytes(content)

        identical, hash1, hash2 = verify_deterministic_output(file1, file2)

        assert identical
        assert hash1 == hash2


class TestEnvironmentValidation:
    """Test environment validation for determinism."""

    def test_validate_with_deterministic_context(self):
        """Test validation passes inside deterministic context."""
        with deterministic(seed=42, strict=True):
            results = validate_deterministic_environment()

            # Should have all checks passing
            assert results["checks"]["pythonhashseed"]
            assert results["checks"]["omp_num_threads"]
            assert results["checks"]["openblas_num_threads"]
            assert results["checks"]["mkl_num_threads"]

    def test_validate_warns_without_context(self):
        """Test validation warns when environment not configured."""
        # Reset environment for this test
        import os

        original = os.environ.pop("PYTHONHASHSEED", None)

        try:
            results = validate_deterministic_environment()

            # Should have warnings
            assert len(results["warnings"]) > 0
            assert results["status"] in ["warning", "pass"]

        finally:
            # Restore
            if original:
                os.environ["PYTHONHASHSEED"] = original

    def test_strict_validation_raises_on_issues(self):
        """Test strict validation raises on issues."""
        import os

        # Clear environment
        original_hashseed = os.environ.pop("PYTHONHASHSEED", None)
        original_omp = os.environ.pop("OMP_NUM_THREADS", None)

        try:
            with pytest.raises(RuntimeError, match="Deterministic environment validation failed"):
                validate_deterministic_environment(strict=True)

        finally:
            # Restore
            if original_hashseed:
                os.environ["PYTHONHASHSEED"] = original_hashseed
            if original_omp:
                os.environ["OMP_NUM_THREADS"] = original_omp


@pytest.mark.integration
class TestManifestDeterminism:
    """Test manifest hash is deterministic across runs.

    Note: The full manifest JSON includes timestamps (generated_at, start_time, end_time)
    which are intentionally non-deterministic. The manifest_hash field excludes these
    timestamps and verifies that the audit configuration and results are reproducible.
    """

    def test_manifest_is_byte_identical(self, tmp_path, simple_config):
        """Test audit manifest hash is deterministic across runs.

        The manifest_hash field verifies reproducibility by excluding timestamps.
        Full manifest JSON will differ due to execution timestamps, which is expected.
        """
        config_path, config = simple_config

        # Load config as AuditConfig object
        config_obj = load_config_from_file(config_path)

        # Run audit twice with same seed
        with deterministic(seed=42):
            # First run
            pipeline1 = AuditPipeline(config_obj)
            results1 = pipeline1.run()

        with deterministic(seed=42):
            # Second run
            pipeline2 = AuditPipeline(config_obj)
            results2 = pipeline2.run()

        # Verify manifest hashes are identical (excludes timestamps)
        manifest1 = results1.execution_info.get("provenance_manifest", {})
        manifest2 = results2.execution_info.get("provenance_manifest", {})

        hash1 = manifest1.get("manifest_hash")
        hash2 = manifest2.get("manifest_hash")

        assert hash1 is not None, "Manifest hash missing from first run"
        assert hash2 is not None, "Manifest hash missing from second run"
        assert hash1 == hash2, f"Manifest hashes differ: {hash1} != {hash2}"

        # Verify other deterministic fields are identical
        assert manifest1.get("configuration", {}).get("hash") == manifest2.get("configuration", {}).get("hash")
        assert manifest1.get("dataset", {}).get("hash") == manifest2.get("dataset", {}).get("hash")
        assert manifest1.get("execution", {}).get("random_seed") == manifest2.get("execution", {}).get("random_seed")


@pytest.mark.integration
@pytest.mark.slow
class TestPDFDeterminism:
    """Test PDF output is byte-identical.

    Note: Full PDF determinism requires WeasyPrint metadata normalization.
    These tests verify the framework is in place.
    """

    def test_pdf_generation_is_consistent(self, tmp_path, simple_config):
        """Test PDF generation produces byte-identical output.

        This validates that PDF generation is deterministic under seeded runs
        with proper metadata normalization.
        """
        config_path, config = simple_config

        # Load config as AuditConfig object
        config_obj = load_config_from_file(config_path)

        pdf1_path = tmp_path / "audit1.pdf"
        pdf2_path = tmp_path / "audit2.pdf"

        # Generate PDFs with same seed
        # Set fixed timestamp for deterministic PDF generation
        import os

        from glassalpha.report.renderer import render_audit_report

        original_source_date_epoch = os.environ.get("SOURCE_DATE_EPOCH")

        try:
            # Set fixed timestamp for reproducible builds
            os.environ["SOURCE_DATE_EPOCH"] = "1577836800"  # 2020-01-01 00:00:00 UTC

            with deterministic(seed=42):
                # First PDF - create fresh pipeline and run
                pipeline1 = AuditPipeline(config_obj)
                results1 = pipeline1.run()
                render_audit_report(results1, output_path=pdf1_path)

            with deterministic(seed=42):
                # Second PDF - create fresh pipeline and run
                pipeline2 = AuditPipeline(config_obj)
                results2 = pipeline2.run()
                render_audit_report(results2, output_path=pdf2_path)

        finally:
            # Restore original environment
            if original_source_date_epoch is None:
                os.environ.pop("SOURCE_DATE_EPOCH", None)
            else:
                os.environ["SOURCE_DATE_EPOCH"] = original_source_date_epoch

        # Verify PDFs are identical
        identical, hash1, hash2 = verify_deterministic_output(pdf1_path, pdf2_path)

        assert identical, f"PDFs differ: {hash1} != {hash2}"


@pytest.mark.integration
class TestCrossPlatformDeterminism:
    """Test determinism across different platforms.

    Note: These tests run in CI across Linux and macOS.
    Windows support is deferred.
    """

    def test_model_predictions_are_platform_independent(self, tiny_binary_data):
        """Test model predictions are same across platforms."""
        X, y = tiny_binary_data

        with deterministic(seed=42):
            model = LogisticRegression(random_state=42, max_iter=100)
            model.fit(X, y)
            predictions = model.predict_proba(X)

            # Save predictions for cross-platform comparison
            # In CI, we'd compare these across different OS runners
            predictions_hash = hash(predictions.tobytes())

            # Basic sanity check
            assert predictions.shape == (100, 2)
            assert np.allclose(predictions.sum(axis=1), 1.0)  # Probabilities sum to 1

    def test_random_forest_is_deterministic(self, tiny_binary_data):
        """Test RandomForest is deterministic with proper seeding."""
        X, y = tiny_binary_data

        with deterministic(seed=42):
            model1 = RandomForestClassifier(random_state=42, n_estimators=10, max_depth=3)
            model1.fit(X, y)
            preds1 = model1.predict_proba(X)

        with deterministic(seed=42):
            model2 = RandomForestClassifier(random_state=42, n_estimators=10, max_depth=3)
            model2.fit(X, y)
            preds2 = model2.predict_proba(X)

        assert np.array_equal(preds1, preds2)


@pytest.mark.integration
@pytest.mark.slow
def test_cli_respects_source_date_epoch(tmp_path):
    """Test CLI uses SOURCE_DATE_EPOCH for deterministic timestamps.

    This test validates that CLI outputs are byte-identical when SOURCE_DATE_EPOCH
    is set, ensuring deterministic behavior across runs and environments.
    """
    import os
    import subprocess

    # Set fixed timestamp for deterministic testing
    env = os.environ.copy()
    env["SOURCE_DATE_EPOCH"] = "1577836800"  # 2020-01-01 00:00:00 UTC

    # Additional determinism controls
    env["PYTHONHASHSEED"] = "42"
    env["OMP_NUM_THREADS"] = "1"
    env["OPENBLAS_NUM_THREADS"] = "1"
    env["MKL_NUM_THREADS"] = "1"

    config = tmp_path / "config.yaml"
    output = tmp_path / "audit.html"

    # Create minimal config
    config.write_text("""
data:
  dataset: german_credit
  target_column: credit_risk
model: {type: logistic_regression}
""")

    # Run CLI multiple times to verify deterministic output
    hashes = []
    for run_num in range(3):
        # Ensure clean state for each run
        if output.exists():
            output.unlink()

        # Run CLI with strict determinism controls
        result = subprocess.run(
            ["glassalpha", "audit", "-c", str(config), "-o", str(output)],
            env=env,
            check=True,
            capture_output=True,
            encoding="utf-8",  # Explicit UTF-8 encoding
        )

        # Verify output was created and has content
        assert output.exists(), f"Output file not created in run {run_num + 1}"
        assert output.stat().st_size > 0, f"Output file empty in run {run_num + 1}"

        # Compute hash
        file_hash = compute_file_hash(output)
        hashes.append(file_hash)

        # Clean up for next run
        output.unlink()

    # Verify all runs produced identical output
    unique_hashes = set(hashes)
    assert len(unique_hashes) == 1, (
        f"CLI outputs not deterministic across runs. Hashes: {hashes}. Expected all hashes to be identical."
    )


@pytest.mark.integration
@pytest.mark.slow
def test_cli_pdf_determinism_with_source_date_epoch(tmp_path):
    """Test CLI PDF generation is deterministic with SOURCE_DATE_EPOCH.

    This test specifically validates that PDF generation produces byte-identical
    output when SOURCE_DATE_EPOCH is set, ensuring deterministic behavior.
    """
    import os
    import subprocess

    # Set fixed timestamp for deterministic testing
    env = os.environ.copy()
    env["SOURCE_DATE_EPOCH"] = "1577836800"  # 2020-01-01 00:00:00 UTC

    # Additional determinism controls
    env["PYTHONHASHSEED"] = "42"
    env["OMP_NUM_THREADS"] = "1"
    env["OPENBLAS_NUM_THREADS"] = "1"
    env["MKL_NUM_THREADS"] = "1"

    config = tmp_path / "config.yaml"
    output = tmp_path / "audit.pdf"

    # Create minimal config
    config.write_text("""
data:
  dataset: german_credit
  target_column: credit_risk
model: {type: logistic_regression}
report:
  output_format: pdf
""")

    # Skip if PDF dependencies not available
    try:
        import weasyprint
    except ImportError:
        pytest.skip("PDF dependencies (weasyprint) not available")

    # Run CLI multiple times to verify deterministic PDF output
    hashes = []
    for run_num in range(2):  # Reduced runs for PDF (slower)
        # Ensure clean state for each run
        if output.exists():
            output.unlink()

        # Run CLI with strict determinism controls
        result = subprocess.run(
            ["glassalpha", "audit", "-c", str(config), "-o", str(output)],
            env=env,
            check=True,
            capture_output=True,
            encoding="utf-8",
        )

        # Verify output was created and has content
        assert output.exists(), f"PDF output file not created in run {run_num + 1}"
        assert output.stat().st_size > 0, f"PDF output file empty in run {run_num + 1}"

        # Compute hash
        file_hash = compute_file_hash(output)
        hashes.append(file_hash)

        # Clean up for next run
        output.unlink()

    # Verify all runs produced identical output
    unique_hashes = set(hashes)
    assert len(unique_hashes) == 1, (
        f"CLI PDF outputs not deterministic across runs. Hashes: {hashes}. Expected all hashes to be identical."
    )
