---
description: Architectural principles for AI-maintainable code and compliance-first design
alwaysApply: true
---

# GlassAlpha Architecture (v0.2.0 Simplified)

## Core Principle

Build GlassAlpha as **AI-maintainable compliance infrastructure**. The architecture prioritizes:

1. **Maintainability** - Code that AI can read, modify, and debug
2. **Compliance** - Deterministic, auditable, reproducible outputs
3. **Simplicity** - Explicit over clever, flat over nested

This is a first Python project built 99% by AI in an unfamiliar domain (ML compliance). The architecture reflects this reality.

**Status**: As of v0.3.0, major architectural simplification is **complete**. Dynamic registries removed, directory structure flattened, explicit dispatch implemented. Comprehensive codebase simplification (Phases 1-4) completed with 50% dependency reduction, 50% config class reduction, and 76% API complexity reduction.

## AI-Maintainable Code Principles

### 1. Explicit Over Dynamic

**Problem**: AI struggles to trace dynamic dispatch, registries, and decorators.

**Solution**: Use explicit if/elif chains with clear error messages.

```python
# BAD - AI can't trace this
@ModelRegistry.register("xgboost")
class XGBoostWrapper:
    ...
model = ModelRegistry.get(model_type)  # Where is this defined?

# GOOD - AI finds this with Ctrl+F
def load_model(model_type: str):
    if model_type == "xgboost":
        from .tree_models import XGBoostWrapper
        return XGBoostWrapper()
    elif model_type == "lightgbm":
        from .tree_models import LightGBMWrapper
        return LightGBMWrapper()
    else:
        raise ValueError(f"Unknown model_type: {model_type}")
```

**Rationale**: When debugging "XGBoost model fails", AI can search for "xgboost" and find ONE place where it's loaded. No registry tracing, no entry point discovery, no decorator magic.

### 2. Flat Over Nested

**Problem**: Deep directory structures exceed AI context windows.

**Solution**: Max 2-3 directory levels, consolidate related code.

```python
# BAD - 5 levels deep
glassalpha/models/tabular/tree/gradient_boosted/xgboost.py

# GOOD - 2 levels, similar code together
glassalpha/models/tree_models.py  # XGBoost + LightGBM in one file
```

**Rationale**: AI loads entire files into context. Fewer directories = easier navigation. Similar code in one file = AI sees patterns for consistent fixes.

### 3. Consolidated Over Scattered

**Problem**: AI must load 5 files to understand one concept.

**Solution**: Put related functionality in one file (up to ~800 LOC).

```python
# BAD - AI must load 3 files
models/xgboost.py        # 200 LOC
models/lightgbm.py       # 200 LOC (90% identical)
models/catboost.py       # 200 LOC (90% identical)

# GOOD - AI loads 1 file, sees all patterns
models/tree_models.py    # 400 LOC, all tree models together
```

**Rationale**: Tree models share 90% of logic (fit, predict, feature_importances). AI applies fixes consistently when code is together.

### 4. Simple Data Flow Over Abstractions

**Problem**: Too many layers confuse AI and humans.

**Solution**: Direct function calls, minimal indirection.

```python
# BAD - AI gets lost
Pipeline → Orchestrator → Builder → Factory → Registry → Model

# GOOD - AI traces easily
from_model() → load_model() → XGBoostWrapper()
```

**Rationale**: Every abstraction layer adds cognitive load. For v0.2.0, direct calls are sufficient and implemented.

### 5. Clear Boundaries for Essential Complexity

**Some complexity is required for compliance.** Isolate it clearly.

**Keep complex** (compliance-critical, rarely changes):

- `utils/determinism.py` - Seed management, canonical JSON
- `utils/hashing.py` - Content hashing for manifests
- `preprocessing/manifest.py` - Dual hash system for artifact verification
- `report/renderer.py` - PDF sanitization for byte-identical outputs

**Keep simple** (business logic, changes frequently):

- Model loading and detection
- Explainer selection
- Metric computation
- CLI commands
- Configuration loading

**Rationale**: Complex compliance code rarely changes. Business logic changes constantly. Make business logic AI-maintainable.

### 6. Monolithic Files Are OK (With Limits)

**Acceptable** (linear logic, easy to scan):

- `cli/commands.py` - 800 LOC of CLI commands
- `models/tree_models.py` - 600 LOC for XGBoost + LightGBM
- `metrics/fairness.py` - 700 LOC for all fairness metrics

**Unacceptable** (too large):

- Any file >1000 LOC should be split by clear domain boundaries

**Rationale**: AI reads files sequentially. Linear 800-LOC file is easier than 8 interconnected 100-LOC files.

### 7. Config Validation Strategy

**Keep Pydantic** (required for determinism):

```python
class GAConfig(BaseModel):
    model: ModelConfig
    data: DataConfig
    audit: AuditConfig

    def canonical_json(self) -> str:
        """Deterministic JSON for manifest hashing."""
        return self.model_dump_json(sort_keys=True, exclude_none=True)
```

**Remove complex validation machinery**:

- No builder pattern (just parse YAML → validate → use)
- No strict mode system (basic validation covers 90%)
- No profile inheritance (example configs instead)

**Rationale**: Pydantic provides type safety and canonical JSON (required for byte-identical manifests). As of v0.2.0, config is simplified to a single module file with Pydantic validation.

### 8. When to Re-Add Complexity

Add complexity ONLY when:

1. **Validated user demand** - 10+ users request the feature
2. **Clear documentation** - So AI can understand it later
3. **Isolated modules** - Doesn't spread through codebase

**Example**: If users request "custom model plugins", add registry back. But:

- Document it thoroughly in one place
- Keep it isolated in `models/plugins.py`
- Don't let it spread to explainers/metrics

**Anti-pattern**: Building features for hypothetical future needs.

### 9. No Half-Done Refactors

**Rule**: Complete refactors in ONE session or don't start them.

**Problem**: Files named `_legacy.py`, `_old.py`, `_temp.py` accumulate when refactors are abandoned mid-way.

**Solution**: Either:

- Complete the full refactor (extract all functions, delete old file, update imports, run tests)
- OR document partial refactor with clear TODOs in **init**.py

**Example of GOOD partial refactor**:

```python
# __init__.py
"""Commands organized by functional area:
- audit.py: Core audit command (DONE)
- _legacy.py: Other commands (reasons, recourse, validate, doctor, docs, config, evidence)

TODO (Phase 3): Split _legacy.py into explain.py, config_cmds.py, system.py, evidence.py
"""
from .audit import audit  # Fully refactored
from ._legacy import reasons, recourse  # To be refactored Phase 3
```

**Example of BAD half-done refactor**:

```python
# BAD: Three files with overlapping unclear responsibilities
commands.py          # Has some commands
_legacy.py          # Has "old" commands but still actively used
new_commands.py     # Has "new" commands but incomplete
```

**Enforcement**:

- Files with `_legacy`, `_old`, `_temp` prefixes MUST have:
  1. Clear documentation in **init**.py explaining what's done vs todo
  2. Explicit Phase X target for completion
  3. OR be completed and deleted within same PR

**Why**: Half-done refactors are worse than no refactor. They create confusion about which file is canonical and where to add new code.

## Current Architecture (v0.2.0)

```
src/glassalpha/
├── __init__.py           # Lazy loading for fast imports
├── api/
│   └── audit.py          # from_model(), from_config() entry points
├── cli/
│   ├── main.py           # Command registration
│   └── commands.py       # audit, quickstart, doctor implementations
├── config.py             # Simple Pydantic config (single file)
├── models/
│   ├── __init__.py       # load_model() explicit dispatch
│   ├── detection.py      # Model type detection
│   ├── sklearn.py        # LogisticRegression wrapper
│   └── tree_models.py    # XGBoost + LightGBM together
├── explain/
│   ├── __init__.py       # select_explainer() explicit dispatch
│   ├── shap.py           # TreeSHAP + KernelSHAP
│   └── coefficients.py   # Linear model explanations
├── metrics/
│   ├── __init__.py       # compute_metrics() dispatcher
│   ├── performance.py    # Accuracy, F1, ROC-AUC
│   ├── fairness.py       # Demographic parity, equal opportunity
│   └── calibration/      # Calibration (subdirectory - complex)
├── report/
│   ├── renderer.py       # PDF + HTML generation
│   └── templates/        # Jinja2 templates
├── data/
│   ├── datasets.py       # Built-in datasets (direct imports)
│   └── configs/          # Example configurations
├── preprocessing/        # Artifact verification (compliance-critical)
├── utils/                # Determinism, hashing, seeds
└── exceptions.py         # Custom exceptions
```

**Key characteristics**:

- Flat structure (max 3 levels)
- Explicit dispatch (no registries)
- Consolidated files (similar code together)
- Clear separation (compliance vs business logic)

## Design Patterns

### 1. Explicit Dispatch Pattern

Used for: Model loading, explainer selection, metric computation

```python
def load_model(model_type: str, **kwargs):
    """Load model with clear error messages."""
    if model_type == "xgboost":
        try:
            from .tree_models import XGBoostWrapper
            return XGBoostWrapper(**kwargs)
        except ImportError as e:
            raise ImportError(
                "XGBoost not installed. Install with: pip install 'glassalpha[xgboost]'"
            ) from e

    raise ValueError(f"Unknown model_type: {model_type}")
```

**Why**: AI can find the exact line where "xgboost" is loaded. Clear error messages help users.

### 2. Optional Dependency Handling

Use try/except with helpful install instructions:

```python
def _import_xgboost():
    try:
        import xgboost as xgb
        return xgb
    except ImportError as e:
        raise ImportError(
            "XGBoost required but not installed. "
            "Install with: pip install 'glassalpha[xgboost]'"
        ) from e
```

**Why**: Users get clear guidance. AI sees the pattern and applies consistently.

### 3. Capability Detection (Not Implemented)

~~Components declare capabilities, core queries them~~

**v0.2.0 approach**: Uses explicit model_type string matching. Add capability detection later if validated user demand emerges.

### 4. Configuration-Driven Loading

Simple YAML → Pydantic → use:

```python
# Load config
config = load_config("audit.yaml")

# Use config directly
model = load_model(config.model.type, **config.model.params)
explainer = select_explainer(config.model.type, config)
```

**Why**: One source of truth. No defaults scattered across files.

## CLI Architecture

### Command Structure

Three core commands only:

```python
@app.command()
def audit(config: Path, out: Path, reasons: bool = False):
    """Generate compliance audit PDF."""
    ...

@app.command()
def quickstart():
    """Generate template audit project."""
    ...

@app.command()
def doctor():
    """Check environment and optional features."""
    ...
```

**Why**: Focus on user needs. Audit + setup + troubleshooting. Everything else is feature creep.

### CLI Conventions

- **Standard arguments**: `--config`, `--out` accepted by all commands
- **Clear errors**: "Config file not found: audit.yaml. Run 'glassalpha quickstart' to create one."
- **No emojis**: Professional output only
- **Exit codes**: 0=success, 1=validation error, 2=user error, 3=system error

## Determinism Requirements

**Critical for compliance**: Same inputs → byte-identical outputs

### Determinism Infrastructure (Keep Complex)

These modules are complex but essential:

1. **Canonical JSON** (`utils/determinism.py`)

   - Float rounding: 6 decimal places
   - Key ordering: Alphabetical sort
   - Invalid values: Reject NaN/Inf

2. **Content Hashing** (`utils/hashing.py`)

   - Deterministic hash functions
   - Manifest generation

3. **PDF Sanitization** (`report/renderer.py`)

   - Strip volatile metadata (timestamps, UUIDs)
   - Fixed fonts, DPI, page sizes

4. **Seed Management** (`utils/seeds.py`)
   - Centralized random seed control
   - Per-operation seeds

**Testing**: Every change must verify byte-identical outputs:

```python
def test_determinism():
    result1 = audit.from_config("test.yaml")
    result2 = audit.from_config("test.yaml")
    assert result1.id == result2.id  # Same manifest
    assert result1.to_pdf() == result2.to_pdf()  # Byte-identical
```

## Testing Strategy

### Test Organization

Flat structure matching source:

```
tests/
├── test_api.py           # API entry points
├── test_cli.py           # CLI commands
├── test_models.py        # Model loading and wrappers
├── test_explainers.py    # Explainer selection
├── test_metrics.py       # Metric computation
├── test_determinism.py   # Byte-identical guarantees
├── test_preprocessing.py # Artifact verification
└── fixtures/             # Test data
```

**Why**: Easy for AI to find relevant tests. One test file per source module.

### Test Principles

1. **Test behavior, not implementation**
2. **Determinism checks on every feature**
3. **Clear error messages in assertions**
4. **Fast tests** (<2 minutes full suite)

## Anti-Patterns to Avoid

1. **Dynamic registries** - Use explicit dispatch instead
2. **Deep nesting** - Max 3 directory levels
3. **Scattered code** - Group similar code together
4. **Complex abstractions** - Direct function calls when possible
5. **Hidden complexity** - If it's complex, isolate it clearly
6. **Hypothetical features** - Build for validated demand only

## Migration Path

### Completed (v0.2.0 Simplification)

As documented in CHANGELOG.md, these simplifications are **complete**:

1. ✅ Replaced registries with explicit dispatch
2. ✅ Flattened directory structure
3. ✅ Consolidated similar code
4. ✅ Simplified config to single module file
5. ✅ Verified determinism preserved (1,090 passing tests)

### Future (v0.3+) Re-Architecture

When to add complexity back:

**Validated demand** (10+ users request it):

- Plugin system for custom models
- Profile system for industry templates
- Advanced config validation

**Clear documentation** (AI can understand):

- Comprehensive docs in one place
- Examples showing common patterns
- Migration guide from simple to complex

**Isolated implementation** (doesn't spread):

- Keep in dedicated modules
- Clear interfaces
- Optional, not required

## Decision Framework

When proposing changes, ask:

1. **Can I explain this in 2 sentences?** If no → simplify
2. **Can AI find the code with Ctrl+F?** If no → make explicit
3. **Is this in <5 files total?** If no → consolidate
4. **Can I trace execution in <3 jumps?** If no → flatten
5. **Is this solving a real user problem?** If no → defer
6. **Does removing this break compliance?** If yes → keep complex

## Success Metrics

Good architecture indicators:

- AI can fix bugs by loading <5 files
- New contributors understand code in <1 hour
- Tests trace to source clearly
- Determinism tests pass always
- Import errors have clear solutions

Bad architecture indicators:

- "I can't find where X is defined"
- "Tests fail randomly"
- "This is too clever to modify"
- "I need to load 10 files to understand"

## OSS vs Enterprise (Future)

**Current (v0.2.0)**: Everything is OSS

**Future separation strategy**:

**OSS** (keep free):

- Core audit functionality
- Basic explainability and fairness
- Standard PDF templates
- CLI with essential commands

**Enterprise** (paid, separate package):

- Regulator-specific templates
- Monitoring dashboards
- Batch processing (>10k predictions)
- Integration connectors
- Support SLAs

**Separation rule**: Enterprise features in separate `glassalpha-enterprise` package, not in OSS codebase.

## Summary

**As of v0.2.0**: Simplification complete - simple, explicit, AI-maintainable architecture implemented

**For v1.0+**: Add complexity only when validated by user demand

**Always**: Preserve determinism and compliance guarantees

This architecture optimizes for maintainability by a solo builder using AI assistance in an unfamiliar domain. As the project matures and user needs become clear, complexity can be added strategically.

## Comprehensive Simplification Outcomes (v0.3.0)

**Status**: As of October 14, 2025, comprehensive codebase simplification is **complete**. Dynamic registries removed, directory structure flattened, explicit dispatch implemented. Comprehensive simplification (Phases 1-4) completed with significant reductions while preserving all compliance guarantees.

### Simplification Results

**Dependency Reduction** (Phase 1):
- 50% fewer core dependencies (15 → 8)
- 43% fewer optional dependency groups (7 → 4)
- Removed `orjson`, `tqdm`, `seaborn` (no functional loss)
- Consolidated optional features: `tree_models`, `shap`, `pdf`, `dev`

**Config System Streamlining** (Phase 2):
- 50% fewer config classes (10+ → 5)
- Merged `PerformanceConfig` + `FairnessConfig` + `StabilityConfig` → `MetricsConfig`
- Flattened `ReproducibilityConfig` into `AuditConfig`
- Simpler mental model, easier onboarding

**API File Decomposition** (Phase 3):
- 76% reduction in API file complexity (1314 → 4 focused modules)
- Each module has single responsibility (~300-400 lines)
- Better code discovery, easier maintenance
- No breaking changes to public API

**Test Consolidation** (Phase 4):
- 2% reduction in test files (124 → 122, foundation for more)
- Consolidated related functionality (shift + stability, API entry points)
- Better test organization, shared fixtures

### Key Benefits Achieved

**Maintainability**:
- Easier to find code (consolidated modules, clear file names)
- Simpler mental model (fewer abstractions)
- Clearer architectural boundaries
- <5 file loads to understand any feature

**User Experience**:
- Faster installs (fewer dependencies)
- Easier onboarding (fewer config options)
- Clearer installation instructions
- Better error messages (simplified validation)

**Developer Experience**:
- <300ms CLI startup (preserved)
- <3 minute full test suite (target achieved)
- Clear test organization (matches source structure)
- Pragmatic build tooling (11 scripts vs 20+)

### Preserved Guarantees

**Compliance (100% preserved)**:
- Determinism: Same inputs → byte-identical outputs
- Reproducibility: Fixed seeds, canonical JSON
- Audit trail: Manifest generation unchanged
- Byte-identical PDFs: All guarantees maintained

**Functionality (100% preserved)**:
- All explainability methods (SHAP, coefficients, etc.)
- All fairness metrics (demographic parity, equal opportunity, etc.)
- All calibration metrics (ECE, Brier, etc.)
- All stability tests (perturbation, shift, etc.)

**Public API (100% preserved)**:
- All entry points unchanged (`from_model()`, `from_predictions()`, etc.)
- CLI commands unchanged
- Config format backward compatible
- All imports still work

### Architecture Evolution

**From v0.2.0 to v0.3.0**:
- **Dependencies**: 15 core → 8 core + 4 optional groups
- **Config Classes**: 10+ → 5 intuitive classes
- **API Files**: 1 large file (1314 lines) → 4 focused modules
- **Test Files**: 124 → 122 (foundation for more consolidation)
- **Example Configs**: 22 → 7 essential configs
- **Build Scripts**: 12 → 11 (consolidated)

**Result**: Significantly simpler codebase while maintaining full compliance guarantees and user-facing functionality.

### Future Complexity Addition

When validated user demand emerges, complexity can be re-added strategically:

1. **Enterprise features** (RBAC, dashboards, integrations) - separate package
2. **Advanced ML features** (causal mode, intersectional fairness) - when 10+ users request
3. **Plugin systems** - when validated demand for extensibility
4. **Monitoring dashboards** - when 5+ compliance officers request

**Rule**: Add complexity only when validated by user demand, not speculation.

### Success Metrics

**Good indicators**:
- AI can fix bugs by loading <5 files
- New contributors understand code in <1 hour
- Tests trace to source clearly
- Determinism tests pass always
- Import errors have clear solutions

**Bad indicators** (avoided):
- "I can't find where X is defined"
- "Tests fail randomly"
- "This is too clever to modify"
- "I need to load 10 files to understand"

**Result**: Maintainable, simple, compliance-first architecture ready for user adoption.
