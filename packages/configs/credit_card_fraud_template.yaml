# GlassAlpha Template: Credit Card Fraud Detection
#
# This is a CUSTOMIZABLE TEMPLATE - not runnable without your data.
#
# Before using:
#   1. Replace data.path with your CSV file path (currently: /path/to/creditcard.csv)
#   2. Update protected_attributes for your domain
#   3. Adjust model parameters as needed
#
# Documentation: https://glassalpha.com/getting-started/custom-data/
# Example Usage: glassalpha audit --config credit_card_fraud_template.yaml

# Credit Card Fraud Detection Configuration
# Dataset: Credit Card Fraud Detection (Kaggle)
# Task: Detect fraudulent transactions in anonymized credit card data
# Class Distribution: Highly imbalanced (~0.17% fraud)
# Size: 284,807 transactions
# Source: https://www.kaggle.com/mlg-ulb/creditcardfraud
#
# Features: V1-V28 (PCA-transformed), Time, Amount
# Target: Class (1=fraud, 0=legitimate)
#
# Note: Due to PCA transformation, features are not directly interpretable.
# SHAP values show relative importance of transformed features.

audit_profile: tabular_compliance

reproducibility:
  random_seed: 42

data:
  dataset: custom # Use custom dataset with path
  path: /path/to/creditcard.csv
  target_column: Class

  # No traditional protected attributes (features are anonymized via PCA)
  # However, you may want to analyze:
  protected_attributes: []

  # Important: Handle class imbalance
  # - Consider stratified sampling for SHAP
  # - May want to undersample majority class for fairness metrics

model:
  type: xgboost
  params:
    objective: binary:logistic
    n_estimators: 200
    max_depth: 6
    learning_rate: 0.1

    # Critical for imbalanced data:
    scale_pos_weight: 577 # Ratio of negatives to positives (~284K/492)

    # Or use class_weight in scikit-learn:
    # class_weight: balanced

    subsample: 0.8
    colsample_bytree: 0.8
    random_state: 42

    # Performance optimization:
    tree_method: hist
    max_bin: 256

explainers:
  strategy: first_compatible
  priority:
    - treeshap
    - kernelshap
  config:
    treeshap:
      max_samples: 1000
      check_additivity: false

metrics:
  performance:
    metrics:
      - accuracy # Not very useful for imbalanced data
      - precision # Critical: % of flagged transactions that are fraud
      - recall # Critical: % of fraud caught
      - f1 # Harmonic mean of precision/recall
      - auc_roc # Area under ROC curve

  # Threshold analysis is critical for fraud detection:
  # - High threshold: Fewer false positives, more false negatives
  # - Low threshold: More false positives, fewer false negatives
  # - Balance based on business cost of false positives vs false negatives

  fairness:
    metrics: []
    # Limited fairness analysis due to anonymized features
    # If you have demographic data, add:
    # - demographic_parity
    # - equal_opportunity

report:
  template: standard_audit
# Fraud Detection Specific Considerations:
#
# 1. Class Imbalance (~0.17% fraud):
#    - Use scale_pos_weight or class_weight
#    - Focus on precision/recall, not accuracy
#    - Consider PR-AUC over ROC-AUC
#
# 2. Business Costs:
#    - False Positive: Decline legitimate transaction (customer frustration)
#    - False Negative: Miss fraud (financial loss)
#    - Optimize threshold based on cost ratio
#
# 3. Feature Interpretation:
#    - V1-V28 are PCA components (not directly interpretable)
#    - Time: Seconds since first transaction
#    - Amount: Transaction amount in Euros
#    - SHAP shows which PCA components matter most
#
# 4. Model Selection:
#    - XGBoost/LightGBM: Best performance for fraud detection
#    - LogisticRegression: Faster but lower recall
#
# 5. Evaluation Metrics Priority:
#    - Recall (catch fraud): 80-90% target
#    - Precision (avoid false alarms): 10-30% typical
#    - F1 Score: Balance between precision/recall
#
# 6. Threshold Tuning:
#    # Example: Optimize for 90% recall
#    from sklearn.metrics import precision_recall_curve
#    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_proba)
#    threshold_90_recall = thresholds[recalls >= 0.90][0]
#
# 7. Production Considerations:
#    - Real-time scoring requirements (<100ms)
#    - Model retraining frequency (fraud patterns evolve)
#    - Monitoring for concept drift
#
# For more information:
# - Original paper: https://www.kaggle.com/mlg-ulb/creditcardfraud
# - Imbalanced learning: https://imbalanced-learn.org/
# - Fraud detection best practices: Focus on business metrics
